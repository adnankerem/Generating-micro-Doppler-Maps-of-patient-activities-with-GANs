{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "from scipy import fftpack\n",
    "from matplotlib.colors import LogNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset file\n",
    "file_name_1 = './data/set1WR14.h5'\n",
    "\n",
    "def h5_file_numpy(filename):\n",
    "    \"\"\"\n",
    "    Reads a .h5 file containing micro-Doppler data, extracts the 'mDoppler' subgroup,\n",
    "    and converts it into a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the .h5 file.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Extracted micro-Doppler data as a NumPy array.\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as hdf_mDoppler_WR14:\n",
    "        # List and display the top-level groups in the HDF5 file\n",
    "        base_items = list(hdf_mDoppler_WR14.items())\n",
    "        print('Subgroups in mDoppler_WR14:', base_items)\n",
    "\n",
    "        # Extract the 'mDoppler' subgroup and convert it to a NumPy array\n",
    "        WR14_mDoppler_numpy = np.array(hdf_mDoppler_WR14.get('mDoppler'))\n",
    "\n",
    "    # Debugging: Display the extracted NumPy array\n",
    "    print(\"Extracted micro-Doppler data:\", WR14_mDoppler_numpy)\n",
    "    \n",
    "    return WR14_mDoppler_numpy\n",
    "\n",
    "# Extract micro-Doppler data for subject 000\n",
    "WR14_mDoppler_subject000_numpy = h5_file_numpy(file_name_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f784a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the timestamp CSV file\n",
    "timestamp_location = './data/timestamp_speech.csv'\n",
    "\n",
    "def miliseconds_past_func(timestamp_location):\n",
    "    \"\"\"\n",
    "    Calculates the milliseconds elapsed since the first timestamp for each entry in the CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    timestamp_location (str): Path to the CSV file containing timestamps.\n",
    "\n",
    "    Returns:\n",
    "    list: Milliseconds elapsed since the first timestamp.\n",
    "    \"\"\"\n",
    "    # Load the timestamp data from the CSV file\n",
    "    df_timestamps = pd.read_csv(timestamp_location)\n",
    "    \n",
    "    # Extract the first timestamp and convert it to milliseconds\n",
    "    first_time = df_timestamps[\"timestamp\"][0]\n",
    "    first_timestamp_milliseconds = datetime.strptime(first_time, '%Y-%m-%dT%H:%M:%S.%f').timestamp() * 1000\n",
    "\n",
    "    # Calculate elapsed time for each subsequent timestamp\n",
    "    milliseconds_past = []\n",
    "    for i in df_timestamps[\"timestamp\"]:\n",
    "        current_time = datetime.strptime(i, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "        millisec = current_time.timestamp() * 1000\n",
    "        milliseconds_past.append(millisec - first_timestamp_milliseconds)\n",
    "\n",
    "    print(\"Milliseconds elapsed since the first timestamp:\", milliseconds_past)\n",
    "    return milliseconds_past\n",
    "\n",
    "# Calculate elapsed milliseconds for subject 000\n",
    "milliseconds_past_subject000 = miliseconds_past_func(timestamp_location)\n",
    "\n",
    "def frame_count(milliseconds_past):\n",
    "    \"\"\"\n",
    "    Calculates the frame counts based on elapsed milliseconds.\n",
    "\n",
    "    Parameters:\n",
    "    milliseconds_past (list): Milliseconds elapsed since the first timestamp.\n",
    "\n",
    "    Returns:\n",
    "    list: Frame counts for each interval.\n",
    "    \"\"\"\n",
    "    frame_count = []\n",
    "    previous_time = 0\n",
    "\n",
    "    for i in milliseconds_past:\n",
    "        # Calculate frame count for the current interval\n",
    "        frame_count.append(int((i - previous_time) // 90))\n",
    "        previous_time = i\n",
    "\n",
    "        # Add a final frame count of 41 frames for the last interval (3.7 seconds optimal sample length)\n",
    "        if milliseconds_past[-1] == i:\n",
    "            frame_count.append(41)\n",
    "\n",
    "    print(\"Frame counts for each interval:\", frame_count)\n",
    "    print(\"Total time covered (seconds):\", sum(frame_count) * 90 / 1000)\n",
    "    return frame_count\n",
    "\n",
    "# Calculate frame counts for subject 000\n",
    "frame_count_subject000 = frame_count(milliseconds_past_subject000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load timestamps from a CSV file\n",
    "def timestamps(TimestampLocation):\n",
    "    df_timestamps = pd.read_csv(TimestampLocation)\n",
    "    return df_timestamps\n",
    "\n",
    "# Load the timestamp data\n",
    "df_timestamps = timestamps(timestamp_location)\n",
    "\n",
    "# Function to prepare the data by processing the Doppler signals\n",
    "def data_prepare(miliseconds_past, WR14_mDoppler_numpy, df_timestamps):\n",
    "    # Initialize variables for action frame calculation and list storage\n",
    "    action_before_ms, remaining_ms, actions_frame_sum, index_counter = 0, 0, 0, 0\n",
    "    dtype_list = []  # List to store Doppler signal segments\n",
    "    mDoppler_list_WR14 = []  # List to store Doppler signal data\n",
    "    mDoppler_labels_WR14 = []  # List to store corresponding labels\n",
    "\n",
    "    # Temporary array for storing data\n",
    "    deneme_numpy_append = np.zeros(41)\n",
    "\n",
    "    # Loop through the miliseconds_past list and process each time step\n",
    "    for i in miliseconds_past:\n",
    "        remaining_action_frame_count = 0  # Initialize remaining action frame count\n",
    "\n",
    "        # Calculate the number of frames since the last action\n",
    "        action_frame_count = int((i - action_before_ms) // 90)\n",
    "        print(f\"Action frame count: {action_frame_count}\")\n",
    "\n",
    "        # Case 1: Action frame count is greater than 40\n",
    "        if action_frame_count > 40:\n",
    "            # Extract a segment of the Doppler signal and store it\n",
    "            mDoppler_list_WR14.append(np.array([WR14_mDoppler_numpy[actions_frame_sum:actions_frame_sum + 40]]))\n",
    "            dtype_list.append(np.array([WR14_mDoppler_numpy[actions_frame_sum + 40:actions_frame_sum + 80]]))\n",
    "            mDoppler_labels_WR14.append(df_timestamps[\"command\"][index_counter])\n",
    "\n",
    "            # If the action frame count exceeds 40, handle the remaining frames\n",
    "            if (action_frame_count - 40) > 40:\n",
    "                remaining_action_frame_count = action_frame_count - 40\n",
    "                mDoppler_list_WR14.append(np.array([WR14_mDoppler_numpy[actions_frame_sum + 40:actions_frame_sum + 80]]))\n",
    "                dtype_list.append(np.array([WR14_mDoppler_numpy[actions_frame_sum + 40:actions_frame_sum + 80]]))\n",
    "                mDoppler_labels_WR14.append(df_timestamps[\"command\"][index_counter])\n",
    "\n",
    "                remaining_action_frame_count -= 40\n",
    "                mDoppler_list_WR14.append(np.array([WR14_mDoppler_numpy[actions_frame_sum + 40 + remaining_action_frame_count:actions_frame_sum + 80 + remaining_action_frame_count]]))\n",
    "                dtype_list.append(np.array([WR14_mDoppler_numpy[actions_frame_sum + 40 + remaining_action_frame_count:actions_frame_sum + 80 + remaining_action_frame_count]]))\n",
    "                mDoppler_labels_WR14.append(df_timestamps[\"command\"][index_counter])\n",
    "\n",
    "            else:\n",
    "                # Handle case where action frame count is <= 40 but > 0\n",
    "                mDoppler_list_WR14.append(np.array([WR14_mDoppler_numpy[actions_frame_sum + (action_frame_count - 40):actions_frame_sum + 40 + (action_frame_count - 40)]]))\n",
    "                dtype_list.append(np.array([WR14_mDoppler_numpy[actions_frame_sum + (action_frame_count - 40):actions_frame_sum + 40 + (action_frame_count - 40)]]))\n",
    "                mDoppler_labels_WR14.append(df_timestamps[\"command\"][index_counter])\n",
    "\n",
    "        # Case 2: Action frame count is less than 40\n",
    "        elif action_frame_count < 40:\n",
    "            empty_rows = 40 - action_frame_count\n",
    "            print(f\"Action frame count < 40, action_frame_count: {action_frame_count}\")\n",
    "            print(f\"Index: {index_counter}\")\n",
    "\n",
    "            # Extract available frames and resize if needed\n",
    "            empty_rows_added = np.array(WR14_mDoppler_numpy[actions_frame_sum:(actions_frame_sum + action_frame_count)])\n",
    "            print(f\"Empty rows added: {empty_rows_added}\")\n",
    "\n",
    "            empty_rows_added_copy = empty_rows_added.copy()\n",
    "            print(f\"Shape before resize: {empty_rows_added_copy.shape}\")\n",
    "\n",
    "            # Resize the array to ensure it has 40 frames\n",
    "            empty_rows_added_copy.resize((40, 128))\n",
    "            print(f\"Shape after resize: {empty_rows_added_copy.shape}\")\n",
    "\n",
    "            mDoppler_list_WR14.append([empty_rows_added_copy])\n",
    "            mDoppler_labels_WR14.append(df_timestamps[\"command\"][index_counter])\n",
    "\n",
    "        # Case 3: Action frame count is exactly 40\n",
    "        else:\n",
    "            mDoppler_list_WR14.append(np.array([WR14_mDoppler_numpy[actions_frame_sum:actions_frame_sum + action_frame_count]]))\n",
    "            mDoppler_labels_WR14.append(df_timestamps[\"command\"][index_counter])\n",
    "\n",
    "        # Update the frame sum for the next iteration\n",
    "        actions_frame_sum += action_frame_count\n",
    "        print(f\"Action before sum: {action_before_ms}\")\n",
    "        index_counter += 1\n",
    "        action_before_ms = i\n",
    "\n",
    "    # Append the final data for the last frame\n",
    "    mDoppler_list_WR14.append(np.array([WR14_mDoppler_numpy[actions_frame_sum:actions_frame_sum + 40]]))\n",
    "    mDoppler_labels_WR14.append(df_timestamps[\"command\"][11])\n",
    "\n",
    "    print(f\"Total frames in mDoppler list: {len(mDoppler_list_WR14)}\")\n",
    "\n",
    "    # Convert the data list to a numpy array for further processing\n",
    "    WR14_mDoppler_numpy_deneme = np.array(mDoppler_list_WR14)\n",
    "    \n",
    "    return [WR14_mDoppler_numpy_deneme, mDoppler_labels_WR14]\n",
    "\n",
    "# Example usage of the function\n",
    "WR14_mDoppler_numpy_subject000_wlabels = data_prepare(milliseconds_past_subject000, WR14_mDoppler_subject000_numpy, df_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the result from the data_prepare function into separate variables\n",
    "# WR14_mDoppler_numpy_subject000 contains the Doppler signal data\n",
    "# WR14_mDoppler_labels_subject000 contains the corresponding labels for the Doppler signals\n",
    "WR14_mDoppler_numpy_subject000 = WR14_mDoppler_numpy_subject000_wlabels[0]\n",
    "WR14_mDoppler_labels_subject000 = WR14_mDoppler_numpy_subject000_wlabels[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to a range between 0 and 1 using min-max normalization\n",
    "# This scales the WR14_mDoppler_numpy_subject000 data by subtracting the minimum value and dividing by the range\n",
    "# This method ensures that the data values are scaled within the [0, 1] interval\n",
    "normalizedData = (WR14_mDoppler_numpy_subject000 - np.min(WR14_mDoppler_numpy_subject000)) / (np.max(WR14_mDoppler_numpy_subject000) - np.min(WR14_mDoppler_numpy_subject000))\n",
    "\n",
    "# Alternatively, you can use linalg.norm for normalization by dividing the data by its Euclidean norm (this was previously commented out)\n",
    "# normalizedData = WR14_mDoppler_numpy_deneme / np.linalg.norm(WR14_mDoppler_numpy_deneme)\n",
    "\n",
    "# Print the first normalized data to check the result\n",
    "print(normalizedData[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 14th element (slice) of the WR14_mDoppler_numpy_subject000 array as an image.\n",
    "# The 'extent' argument specifies the axis limits (x-axis from 0 to 40, y-axis from 0 to 128).\n",
    "# The 'aspect' is set to 'auto' to let matplotlib automatically adjust the aspect ratio.\n",
    "plt.imshow(WR14_mDoppler_numpy_subject000[14].T, extent=[0, 40, 0, 128], aspect='auto')\n",
    "\n",
    "# Display the shape of the first element of WR14_mDoppler_numpy_subject000 to understand its dimensions.\n",
    "print(WR14_mDoppler_numpy_subject000[0].shape)\n",
    "\n",
    "# Print the type of the first element to verify that it is a numpy array.\n",
    "print(f\"Type of WR14_mDoppler_numpy_subject000[0]: {type(WR14_mDoppler_numpy_subject000[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available for GPU computation. If it is, set the device to 'cuda'; otherwise, use 'cpu'.\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# If CUDA is available, print a confirmation message\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "\n",
    "# Set the device for PyTorch operations based on the availability of CUDA (GPU) or fall back to CPU.\n",
    "device = torch.device(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac68a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of labels into a set to get unique values for the WR14_mDoppler_labels_subject000 dataset.\n",
    "# This will help us identify all unique labels present in the dataset.\n",
    "mDoppler_labels_WR14_set = set(WR14_mDoppler_labels_subject000)\n",
    "\n",
    "# Print the unique labels and the total count of unique labels\n",
    "print(f\"Unique labels in WR14_mDoppler_labels_subject000: {mDoppler_labels_WR14_set}\")\n",
    "print(f\"Number of unique labels: {len(mDoppler_labels_WR14_set)}\")\n",
    "\n",
    "# Initialize an empty list to store the counter for each label's occurrence\n",
    "label_list = []\n",
    "\n",
    "# Loop through each label in WR14_mDoppler_labels_subject000\n",
    "# For each label, compare it with all unique labels from the set\n",
    "for labels in WR14_mDoppler_labels_subject000:\n",
    "    counter = 1\n",
    "    # Loop through the unique labels set and check for a match with the current label\n",
    "    for set_index in mDoppler_labels_WR14_set:\n",
    "        if labels == set_index:\n",
    "            label_list.append(counter)\n",
    "        counter += 1\n",
    "\n",
    "# Print the total length of new_list and its contents\n",
    "print(f\"Length of label_list: {len(label_list)}\")\n",
    "print(f\"Contents of label_list: {label_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e28ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the WR14_mDoppler_numpy_subject000 (a NumPy array) into a PyTorch tensor\n",
    "# This allows you to leverage PyTorch's GPU acceleration and tensor operations\n",
    "tensor_x = torch.Tensor(WR14_mDoppler_numpy_subject000)\n",
    "\n",
    "# Optional: Loop through the elements of my_x and print the length of each element (if needed for debugging)\n",
    "# Uncomment the following lines if you want to inspect the length of each element in my_x\n",
    "# for i in my_x:\n",
    "#     print(len(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f94fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "# Normalize data and prepare it for use with PyTorch DataLoader\n",
    "my_x = normalizedData\n",
    "\n",
    "# Prepare the labels from new_list and reshape them into a 2D array (224, 1)\n",
    "my_y = np.array(label_list)\n",
    "my_y = my_y.reshape((len(label_list), 1))\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "tensor_x = torch.Tensor(my_x)\n",
    "tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "# Create a TensorDataset from the input features and labels\n",
    "my_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "# Initialize a DataLoader to handle batching and shuffling of the dataset\n",
    "my_dataloader = DataLoader(my_dataset)\n",
    "\n",
    "# Access the first sample in the dataset to check its shape\n",
    "n_samples_deneme, n_features_deneme = my_dataset[0]\n",
    "\n",
    "# Fetch a specific sample (index 3) from the dataset\n",
    "image, label = my_dataset.__getitem__(3)\n",
    "\n",
    "# Fetch the first sample to check the format of the data\n",
    "aa = my_dataset.__getitem__(0)\n",
    "\n",
    "# Print the label of the first sample\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef33757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size for loading data\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Create a DataLoader instance for batching and shuffling the dataset\n",
    "# The `num_workers` argument is set to use the number of CPU cores available for data loading\n",
    "dataloader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import time as time\n",
    "import numpy as np\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the first image and its corresponding label from the dataset\n",
    "image, label = my_dataset.__getitem__(1)\n",
    "\n",
    "# Display the image (squeeze to remove the channel dimension if it exists and transpose to match the correct orientation)\n",
    "plt.imshow(image.squeeze(1).T)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print the image size and label for debugging\n",
    "# print(\"Image size:\", image.shape)\n",
    "# print(\"Label:\", label)\n",
    "\n",
    "# Check the type of the dataset and inspect its length and the properties of the fetched sample\n",
    "print(f\"Dataset type: {type(my_dataset)}\")\n",
    "print(f\"Dataset length: {len(my_dataset)}\")  # Print the number of samples in the dataset\n",
    "print(f\"Image size: {image.size()}\")  # Print the size of the image tensor\n",
    "print(f\"First sample: {my_dataset.__getitem__(1)}\")  # Print the first sample from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_generator_images=[]\n",
    "my_discriminator_results=[]\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the input size and initialize the layers\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Fully connected layer network\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256*10*32, bias=False),  # Create a layer with the specified input size and output size (256*10*32)\n",
    "            nn.BatchNorm1d(256*10*32),  # Batch normalization to improve training\n",
    "            nn.Softmax()  # Apply Softmax activation (no need for dim argument here)\n",
    "        )\n",
    "\n",
    "        # Transposed convolution layers for upsampling\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 5, bias=False, padding=2),  # Upsample to 128 channels\n",
    "            nn.BatchNorm2d(128),  # Batch normalization for stability\n",
    "            nn.Softmax(),  # Apply Softmax activation (no need for dim argument here)\n",
    "            nn.ConvTranspose2d(128, 64, 5, stride=2, bias=False, padding=2, output_padding=1),  # Upsample to 64 channels\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.Softmax(),  # Apply Softmax activation (no need for dim argument here)\n",
    "            nn.ConvTranspose2d(64, 1, 5, stride=2, bias=False, padding=2, output_padding=1),  # Final output layer (1 channel)\n",
    "            nn.Sigmoid()  # Apply Sigmoid activation to constrain output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the fully connected network\n",
    "        y = self.fc_net(x)\n",
    "        \n",
    "        # Reshape the output from the FC network to match the expected input for the convolutional layers\n",
    "        y = y.reshape((-1, 256, 10, 32))\n",
    "        \n",
    "        # Pass the reshaped tensor through the convolutional layers\n",
    "        y = self.conv_model(y)\n",
    "        \n",
    "        # Return the generated image\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the model layers based on their types.\n",
    "    This function applies different initialization techniques depending on the layer type.\n",
    "\n",
    "    Args:\n",
    "        m (nn.Module): A PyTorch module (layer) whose weights are being initialized.\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__  # Get the class name of the layer (e.g., 'Conv2d', 'Linear', 'BatchNorm')\n",
    "\n",
    "    # Initialize weights for Convolutional layers\n",
    "    if classname.find('Conv') != -1:\n",
    "        # Xavier uniform initialization for convolutional layers\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    \n",
    "    # Initialize weights for Linear layers\n",
    "    elif classname.find('Linear') != -1:\n",
    "        # Xavier uniform initialization for fully connected (linear) layers\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    \n",
    "    # Initialize weights for BatchNorm layers\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        # Normal initialization for BatchNorm weights, mean = 1.0, std = 0.2\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.2)\n",
    "        # Initialize the biases to zero for BatchNorm layers\n",
    "        nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e264f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the generator with an input size of 100\n",
    "generator = Generator(100)\n",
    "\n",
    "# Apply the custom weight initialization to the generator model\n",
    "generator.apply(weights_init)\n",
    "\n",
    "# Generate random noise to feed into the generator (10 samples, 100-dimensional noise vector)\n",
    "noise = torch.normal(0, 1, [10, 100])\n",
    "print(\"Noise shape:\", noise.shape)\n",
    "\n",
    "# Generate the images from the noise vector using the generator\n",
    "generated_image = generator(noise).detach()  # Use .detach() to avoid tracking gradients for this step\n",
    "print(\"Generated image shape:\", generated_image.shape)\n",
    "\n",
    "# Display the first generated image (squeeze to remove any unnecessary dimensions)\n",
    "plt.imshow(generated_image.squeeze()[0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Optionally print the generated image tensor to inspect its values\n",
    "print(generated_image.squeeze()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator class for the GAN model.\n",
    "    This model distinguishes between real and fake images by passing the input through\n",
    "    several convolutional layers followed by a fully connected layer.\n",
    "\n",
    "    Attributes:\n",
    "        model (nn.Sequential): A sequence of layers that make up the discriminator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Discriminator with convolutional layers followed by a linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # First convolutional layer: Converts 1-channel input to 64 channels\n",
    "            nn.Conv2d(1, 64, 5, stride=2, padding=2),\n",
    "            nn.Softmax(dim=1),  # Softmax activation along the channel dimension\n",
    "            nn.Dropout(0.4),    # Dropout to prevent overfitting\n",
    "\n",
    "            # Second convolutional layer: Increases depth from 64 to 128 channels\n",
    "            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.Softmax(dim=1),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            # Third convolutional layer: Keeps depth at 128 channels\n",
    "            nn.Conv2d(128, 128, 5, stride=2, padding=2),\n",
    "            nn.Softmax(dim=1),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            # Flatten the output of the last convolutional layer before passing to the linear layer\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(10240, 14)  # Final linear layer to output a prediction (14 categories in this case)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the Discriminator.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): The input image tensor to the discriminator.\n",
    "\n",
    "        Returns:\n",
    "            tensor: The output of the discriminator after passing through the layers.\n",
    "        \"\"\"\n",
    "        y = self.model(x)\n",
    "        my_discriminator_results.append(y)  # Store the results for later inspection (optional)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Discriminator model\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Apply the weights initialization function to the model\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "# Pass the generated image through the discriminator to get the decision (real/fake classification)\n",
    "decision = discriminator(generated_image)\n",
    "\n",
    "# Print the decision made by the discriminator\n",
    "print(decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for the Discriminator\n",
    "cross_entropy = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Lists to store loss values for analysis\n",
    "Generator_loss_list = []\n",
    "Discriminator_loss_list = []\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, device):\n",
    "    \"\"\"\n",
    "    Compute the loss for the Discriminator, which includes the real and fake loss components.\n",
    "    \n",
    "    Args:\n",
    "        real_output (Tensor): The output of the discriminator for real images.\n",
    "        fake_output (Tensor): The output of the discriminator for generated (fake) images.\n",
    "        device (torch.device): The device used for tensor computations (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        total_loss (Tensor): The total loss, combining real and fake loss.\n",
    "    \"\"\"\n",
    "    # Loss for real images (the discriminator should classify real images as 1)\n",
    "    real_loss = cross_entropy(real_output, torch.ones_like(real_output, device=device))\n",
    "    print(\"Discriminator Real Loss:\", real_loss)\n",
    "\n",
    "    # Loss for fake images (the discriminator should classify fake images as 0)\n",
    "    fake_loss = cross_entropy(fake_output, torch.zeros_like(fake_output, device=device))\n",
    "    print(\"Discriminator Fake Loss:\", fake_loss)\n",
    "\n",
    "    # Total loss is the sum of real and fake losses\n",
    "    total_loss = real_loss + fake_loss\n",
    "    print(\"Discriminator Total Loss:\", total_loss)\n",
    "\n",
    "    # Append the losses for future analysis\n",
    "    Discriminator_loss_list.append([real_loss, fake_loss, total_loss])\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6976719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for the Generator\n",
    "def generator_loss(fake_output, device):\n",
    "    \"\"\"\n",
    "    Compute the loss for the Generator, which encourages the Generator to produce outputs that the Discriminator classifies as real.\n",
    "    \n",
    "    Args:\n",
    "        fake_output (Tensor): The output of the discriminator for generated (fake) images.\n",
    "        device (torch.device): The device used for tensor computations (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        gen_loss (Tensor): The loss for the Generator.\n",
    "    \"\"\"\n",
    "    # Generator loss (the Generator tries to fool the Discriminator by producing images classified as real)\n",
    "    gen_loss = cross_entropy(fake_output, torch.ones_like(fake_output, device=device))\n",
    "    print(\"Generator Loss:\", gen_loss)\n",
    "\n",
    "    # Append the Generator loss for future analysis\n",
    "    Generator_loss_list.append([gen_loss])\n",
    "\n",
    "    return gen_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer setup for the Generator and Discriminator\n",
    "gen_opt = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "dis_opt = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Random seed for reproducibility of the generated images\n",
    "seed = torch.randn([num_examples_to_generate, noise_dim], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for tracking real and fake outputs (argmax values) during training\n",
    "argmax_real = []\n",
    "argmax_fake = []\n",
    "\n",
    "# Training step function for each batch\n",
    "def train_step(images, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt):\n",
    "    # Generate random noise input for the Generator\n",
    "    noise = torch.randn([BATCH_SIZE, noise_dim], device=device)\n",
    "    \n",
    "    # Generate images using the Generator\n",
    "    generated_images = generator(noise)\n",
    "    \n",
    "    # Pass real images through the Discriminator\n",
    "    real_output = discriminator(images)\n",
    "    \n",
    "    # Pass generated (fake) images through the Discriminator (detached to avoid updating Generator)\n",
    "    fake_output = discriminator(generated_images.detach())\n",
    "    \n",
    "    # Track argmax of real and fake outputs for analysis or visualization\n",
    "    argmax_real.append(torch.argmax(real_output, dim=1))\n",
    "    argmax_fake.append(torch.argmax(fake_output, dim=1))\n",
    "    \n",
    "    # Calculate the loss for the Discriminator\n",
    "    disc_loss = discriminator_loss(real_output, fake_output, device)\n",
    "    \n",
    "    # Update Discriminator's weights\n",
    "    dis_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    dis_opt.step()\n",
    "\n",
    "    # Calculate the Generator's loss using the Discriminator's output on generated images\n",
    "    fake_output = discriminator(generated_images)\n",
    "    gen_loss = generator_loss(fake_output, device)\n",
    "    \n",
    "    # Update Generator's weights\n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "\n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, boolean, discri):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate predictions from the model without updating weights\n",
    "        predictions = model(test_input).detach().cpu() * 250\n",
    "\n",
    "    # Create a grid of the generated images and display\n",
    "    grid = make_grid(predictions, 4).numpy().squeeze().transpose(1, 2, 0)\n",
    "    plt.imshow(grid.astype(np.uint16), interpolation='bilinear', aspect='auto')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save the generated image\n",
    "    plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    if boolean:\n",
    "        # If boolean is True, evaluate the generated images with the discriminator\n",
    "        predictions32 = predictions[:32].to(device)\n",
    "        label_prediction = discri(predictions32)\n",
    "\n",
    "        # Extract the predicted labels by taking the argmax\n",
    "        list_label_predictions = [torch.argmax(i) for i in label_prediction]\n",
    "        \n",
    "        return list_label_predictions, predictions\n",
    "    \n",
    "    # Return to training mode\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, epochs, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt):\n",
    "    gloss = []\n",
    "    dloss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        gen_losses = []\n",
    "        disc_losses = []\n",
    "        \n",
    "        for image_batch, _ in dataloader:\n",
    "            image_batch = image_batch.to(device)\n",
    "            gen_loss, disc_loss = train_step(image_batch, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt)\n",
    "            \n",
    "            gen_losses.append(gen_loss.detach().cpu())\n",
    "            disc_losses.append(disc_loss.detach().cpu())\n",
    "\n",
    "        gloss.append(np.mean(gen_losses))\n",
    "        dloss.append(np.mean(disc_losses))\n",
    "\n",
    "        # Produce and save images during training\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed, False, discriminator)\n",
    "\n",
    "        print(f'Time for epoch {epoch + 1} is {time.time() - start:.2f} sec')\n",
    "\n",
    "    # Generate and save images after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed, False, discriminator)\n",
    "\n",
    "    return gloss, dloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33ef56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Move the generator and discriminator models to the specified device (GPU or CPU)\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 100# Number of epochs for training (adjusted from 400)\n",
    "noise_dim = 100  # Dimension of the random noise vector input to the Generator\n",
    "num_examples_to_generate = 32  # Number of generated examples to display per epoch\n",
    "\n",
    "# Train the models for the specified number of epochs and store the loss values\n",
    "gloss, dloss = train(dataloader, EPOCHS, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8f3a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_predictions_generate_and_save, predictions_generate_and_save = generate_and_save_images(\n",
    "    generator,\n",
    "    EPOCHS,\n",
    "    seed,\n",
    "    True,\n",
    "    discriminator\n",
    ")\n",
    "\n",
    "print(label_predictions_generate_and_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84be2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the set to a list for indexing\n",
    "mDoppler_labels_list = list(mDoppler_labels_WR14_set)\n",
    "\n",
    "# Initialize an empty list to store label strings\n",
    "label_list_strings = []\n",
    "\n",
    "# Iterate through the first 32 predictions\n",
    "for idx in range(32):\n",
    "    # Find the label corresponding to the predicted class\n",
    "    predicted_label = label_predictions_generate_and_save[idx].detach().cpu().tolist()\n",
    "    \n",
    "    for label_idx, label_name in enumerate(mDoppler_labels_list):\n",
    "        if predicted_label == label_idx:\n",
    "            label_list_strings.append(label_name)\n",
    "            break\n",
    "    \n",
    "    # Plot the generated image\n",
    "    plt.figure()\n",
    "    plt.imshow(predictions_generate_and_save[idx].detach().cpu().T, aspect='auto')\n",
    "    \n",
    "    # Set the title of the image to the predicted label\n",
    "    plt.title(mDoppler_labels_list[predicted_label])\n",
    "    \n",
    "    # Save the generated and labeled image\n",
    "    plt.savefig(f'generated_and_labeled_image_{idx}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47484d4-e42f-4e35-8d87-3f311cbba96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_list_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260584d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the counter for labeling\n",
    "counter = 0\n",
    "\n",
    "# Function to plot the Fourier spectrum with a logarithmic colormap\n",
    "def plot_spectrum(im_fft):\n",
    "    \"\"\"\n",
    "    Plots the Fourier transform of an image with a logarithmic color map.\n",
    "    \"\"\"\n",
    "    plt.imshow(np.abs(im_fft), norm=LogNorm(vmin=5))  # Using a logarithmic scale for visualization\n",
    "    plt.colorbar()\n",
    "\n",
    "# Iterate through the predictions\n",
    "for im in predictions_generate_and_save:\n",
    "    # Convert image to numpy array and compute FFT\n",
    "    im = im.detach().cpu().numpy().squeeze()\n",
    "    im_fft = fftpack.fft2(im)\n",
    "\n",
    "    # Plot the original Fourier transform\n",
    "    plt.figure()\n",
    "    plot_spectrum(im_fft)\n",
    "    plt.title('Fourier Transform')\n",
    "\n",
    "    # Make a copy of the Fourier transform for filtering\n",
    "    im_fft_filtered = im_fft.copy()\n",
    "\n",
    "    # Get the dimensions of the Fourier transform\n",
    "    rows, cols = im_fft_filtered.shape\n",
    "\n",
    "    # Define the fraction of coefficients to keep (only outer frequencies)\n",
    "    keep_fraction = 0.2\n",
    "\n",
    "    # Set the middle frequencies to zero (filtering)\n",
    "    im_fft_filtered[int(rows * keep_fraction):int(rows * (1 - keep_fraction))] = 0\n",
    "    im_fft_filtered[:, int(cols * keep_fraction):int(cols * (1 - keep_fraction))] = 0\n",
    "\n",
    "    # Plot the filtered Fourier spectrum\n",
    "    plt.figure()\n",
    "    plot_spectrum(im_fft_filtered)\n",
    "    plt.title('Filtered Spectrum')\n",
    "\n",
    "    # Reconstruct the denoised image from the filtered Fourier transform\n",
    "    im_new = fftpack.ifft2(im_fft_filtered).real\n",
    "\n",
    "    # Plot the reconstructed image\n",
    "    plt.figure()\n",
    "    plt.imshow(im_new.T, aspect='auto')  # Display the reconstructed image\n",
    "    plt.title(label_list_strings[counter])  # Set the title to the label\n",
    "\n",
    "    # Save the reconstructed image with the label\n",
    "    plt.savefig(f'generated_and_labeled_image_noise_filtered_{counter}.png')  # Save the image\n",
    "    plt.show()  # Ensure the plot is shown\n",
    "\n",
    "    # Increment the counter for the next image\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot with a larger figure size for better visibility\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot the generator loss\n",
    "plt.plot(gloss, label=\"Generator Loss\", color='blue')\n",
    "\n",
    "# Plot the discriminator loss\n",
    "plt.plot(dloss, label=\"Discriminator Loss\", color='red')\n",
    "\n",
    "# Add a legend to the plot to distinguish between the two loss curves\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de2c6a-3976-4ca9-8fd8-81699e6dbd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
